{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6edafac-59ac-4c9a-a546-28a1ce5017ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, ZeroPadding2D\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "import keras.backend as K \n",
    "from sklearn.metrics import log_loss\n",
    "from custom_layers.scale_layer import Scale\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import random\n",
    "import cv2\n",
    "import math\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split   \n",
    "from keras.layers import Concatenate\n",
    "from tensorflow.keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7f5f3b-2bdd-44db-af8e-3ddd4895f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet121_model(img_rows, img_cols, color_type=1, nb_dense_block=4, growth_rate=32, nb_filter=64, reduction=0.5, dropout_rate=0.0, weight_decay=1e-4, num_classes=None):\n",
    "    '''\n",
    "    DenseNet 121 Model for Keras\n",
    "\n",
    "    Model Schema is based on \n",
    "    https://github.com/flyyufelix/DenseNet-Keras\n",
    "\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    '''\n",
    "\n",
    "    # Handle Dimension Ordering for different backends\n",
    "    global concat_axis\n",
    "    print(\"img dims\")\n",
    "    print(img_rows,img_cols)\n",
    "    img_input = Input(shape=(img_rows, img_cols, color_type), name='data')\n",
    "    concat_axis = 3\n",
    "\n",
    "\n",
    "    # From architecture for ImageNet (Table 1 in the paper)\n",
    "    nb_filter = 64\n",
    "    nb_layers = [6,12,24,16] # For DenseNet-121\n",
    "\n",
    "    # Initial convolution\n",
    "    x = Convolution2D(filters=nb_filter, kernel_size=(7, 7), strides=(2, 2), padding='valid', \n",
    "           use_bias=False, name='conv1')(img_input)\n",
    "    x = BatchNormalization(axis=concat_axis)(x)\n",
    "    x = Scale(axis=concat_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        stage = block_idx+2\n",
    "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate)\n",
    "\n",
    "        # Add transition_block\n",
    "        x = transition_block(x, stage, nb_filter, dropout_rate=dropout_rate)\n",
    "        nb_filter = int(nb_filter)\n",
    "\n",
    "    final_stage = stage + 1\n",
    "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate)\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis)(x)\n",
    "    x = Scale(axis=concat_axis )(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x_fc = GlobalAveragePooling2D()(x)\n",
    "    x_fc = Dense(1000)(x_fc)\n",
    "    x_fc = Activation('softmax')(x_fc)\n",
    "\n",
    "    model = Model(img_input, x_fc)\n",
    "\n",
    "    # The method below works since pre-trained weights are stored in layers but not in the model\n",
    "    x_newfc = GlobalAveragePooling2D()(x)\n",
    "    x_newfc = Dense(num_classes)(x_newfc)\n",
    "    x_newfc = Activation('softmax')(x_newfc)\n",
    "\n",
    "    model = Model(img_input, x_newfc)\n",
    "\n",
    "    # Learning rate is changed to 0.001\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a4b600-c33d-4190-9325-99e16fe833ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, stage, branch, nb_filter, dropout_rate=None):\n",
    "    '''Apply BatchNorm, Relu, bottleneck 1x1 Conv2D, 3x3 Conv2D, and option dropout '''\n",
    "\n",
    "\n",
    "    # 1x1 Convolution (Bottleneck layer)\n",
    "    inter_channel = nb_filter * 4  \n",
    "    x = BatchNormalization(axis=concat_axis)(x)\n",
    "    x = Scale(axis=concat_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(inter_channel, 1, 1)(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # 3x3 Convolution\n",
    "    x = BatchNormalization(axis=concat_axis)(x)\n",
    "    x = Scale(axis=concat_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = ZeroPadding2D((1, 1))(x)\n",
    "    x = Convolution2D(nb_filter, 3, 3)(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def transition_block(x, stage, nb_filter, dropout_rate=None):\n",
    "    ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout  '''\n",
    "\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis)(x)\n",
    "    x = Scale(axis=concat_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(int(nb_filter), 1, 1)(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, grow_nb_filters=True):\n",
    "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
    "        # Arguments\n",
    "            x: input tensor\n",
    "            stage: index for dense block\n",
    "            nb_layers: the number of layers of conv_block to append to the model.\n",
    "            nb_filter: number of filters\n",
    "            growth_rate: growth rate\n",
    "            grow_nb_filters: flag to decide to allow number of filters to grow\n",
    "    '''\n",
    "\n",
    "    concat_feat = x\n",
    "    print(\"debug_line2\")\n",
    "    print(concat_feat)\n",
    "    print(x)\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        branch = i + 1\n",
    "        concat_feat = Concatenate(axis=concat_axis)([concat_feat, x])\n",
    "        if grow_nb_filters:\n",
    "            nb_filter += growth_rate\n",
    "    x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate)\n",
    "\n",
    "    return concat_feat, nb_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c96bbfb-c54c-4605-9c78-38b9f1a3a0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\\Train\\Lab\n",
      "Train\\Train\\Lab\n",
      "Train\\Train\\Lab\n",
      "Train\\Train\\Lab\n",
      "Train\\Train\\Lab\n",
      "Train\\Train\\Lab\n",
      "Train\\Train\\Lab\n",
      "Train\\Train\\Lab\n",
      "Train\\Train\\Lab\n",
      "Train\\Train\\Lab\n",
      "Train\\Train\\Lab\n",
      "Train\\Train\\Lab\n",
      "Train\\Train\\Lab\n",
      "Train\\Train\\Lab\n",
      "Train\\Train\\Lab\n",
      "Train\\Train\\Lab\n",
      "Train\\Train\\Truck\n",
      "Train\\Train\\Truck\n",
      "Train\\Train\\Truck\n",
      "Train\\Train\\Truck\n",
      "Train\\Train\\Truck\n",
      "Train\\Train\\Truck\n",
      "Train\\Train\\Truck\n",
      "Train\\Train\\Truck\n",
      "Train\\Train\\Truck\n",
      "Train\\Train\\Truck\n",
      "Train\\Train\\Truck\n",
      "Train\\Train\\Truck\n",
      "Train\\Train\\Truck\n",
      "Train\\Train\\Truck\n",
      "Train\\Train\\Truck\n",
      "Train\\Train\\Truck\n",
      "Train\\Train\\Truck\n",
      "Loaded 33 training images successfully.\n",
      "Loaded 20 test images successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_images_and_labels(base_dirs, target_size=(128, 128)):\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    label_dict = {base_dir: idx for idx, base_dir in enumerate(base_dirs)}\n",
    "    for base_dir in base_dirs:\n",
    "        if os.path.isdir(base_dir):\n",
    "            for file_name in sorted(os.listdir(base_dir)):\n",
    "                file_path = os.path.join(base_dir, file_name)\n",
    "                if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img = image.load_img(file_path, target_size=target_size)\n",
    "                    img_array = image.img_to_array(img)\n",
    "                    if \"Test\\Test\" not in base_dir:  # Train set\n",
    "                        print(base_dir)\n",
    "                        X_train.append(img_array)\n",
    "                        Y_train.append(label_dict[base_dir])\n",
    "                    else:  # Test set\n",
    "                        X_test.append(img_array)\n",
    "                        Y_test.append(label_dict[base_dir])\n",
    "        else:\n",
    "            print(f\"Not a directory: {base_dir}\")\n",
    "\n",
    "    if not X_train:\n",
    "        print(\"No training images found after processing.\")\n",
    "    else:\n",
    "        print(f\"Loaded {len(X_train)} training images successfully.\")\n",
    "\n",
    "    if not X_test:\n",
    "        print(\"No test images found after processing.\")\n",
    "    else:\n",
    "        print(f\"Loaded {len(X_test)} test images successfully.\")\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = to_categorical(np.array(Y_train), num_classes=len(base_dirs))\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = to_categorical(np.array(Y_test), num_classes=len(base_dirs))\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "# Example usage:\n",
    "folder_paths = [\n",
    "    # Training folders\n",
    "    #r\"Train\\Train\\Case\", \n",
    "    #r\"Train\\Train\\Ho_Entrance\", \n",
    "    #r\"Train\\Train\\Ho_Museum\", \n",
    "    #r\"Train\\Train\\McGregory_Lounge\" \n",
    "    r\"Train\\Train\\Lab\", \n",
    "    r\"Train\\Train\\Truck\",\n",
    "    # Test folder \n",
    "    #r\"Test\\Test\\Case\", \n",
    "    #r\"Test\\Test\\Ho_Entrance\", \n",
    "    #r\"Test\\Test\\Ho_Museum\", \n",
    "    r\"Test\\Test\\Lab\", \n",
    "    r\"Test\\Test\\Truck\"\n",
    "]\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = load_images_and_labels(folder_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a11a30b4-6a78-4461-b1ce-c2761187ad50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img dims\n",
      "128 128\n",
      "debug_line2\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 30, 30, 64), dtype=tf.float32, name=None), name='max_pooling2d_1/MaxPool:0', description=\"created by layer 'max_pooling2d_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 30, 30, 64), dtype=tf.float32, name=None), name='max_pooling2d_1/MaxPool:0', description=\"created by layer 'max_pooling2d_1'\")\n",
      "debug_line2\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 15, 15, 256), dtype=tf.float32, name=None), name='average_pooling2d_3/AvgPool:0', description=\"created by layer 'average_pooling2d_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 15, 15, 256), dtype=tf.float32, name=None), name='average_pooling2d_3/AvgPool:0', description=\"created by layer 'average_pooling2d_3'\")\n",
      "debug_line2\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 7, 7, 640), dtype=tf.float32, name=None), name='average_pooling2d_4/AvgPool:0', description=\"created by layer 'average_pooling2d_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 7, 7, 640), dtype=tf.float32, name=None), name='average_pooling2d_4/AvgPool:0', description=\"created by layer 'average_pooling2d_4'\")\n",
      "debug_line2\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 3, 3, 1408), dtype=tf.float32, name=None), name='average_pooling2d_5/AvgPool:0', description=\"created by layer 'average_pooling2d_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 3, 3, 1408), dtype=tf.float32, name=None), name='average_pooling2d_5/AvgPool:0', description=\"created by layer 'average_pooling2d_5'\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 22\u001b[0m\n\u001b[0;32m     15\u001b[0m callbacks_list \u001b[38;5;241m=\u001b[39m [checkpoint]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Start Fine-tuning\u001b[39;00m\n\u001b[0;32m     17\u001b[0m history\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train,\n\u001b[0;32m     18\u001b[0m           batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     19\u001b[0m           epochs\u001b[38;5;241m=\u001b[39mnb_epoch,\n\u001b[0;32m     20\u001b[0m           shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m           verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m---> 22\u001b[0m           validation_data\u001b[38;5;241m=\u001b[39m(\u001b[43mX_valid\u001b[49m, Y_valid),\n\u001b[0;32m     23\u001b[0m           callbacks\u001b[38;5;241m=\u001b[39mcallbacks_list,\n\u001b[0;32m     24\u001b[0m           )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     27\u001b[0m predictions_valid \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_valid, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_valid' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    img_rows, img_cols = 128,128\n",
    "    channel = 3\n",
    "    num_classes =1\n",
    "    batch_size = 16 \n",
    "    nb_epoch = 10\n",
    "\n",
    "\n",
    "    # Load our model\n",
    "    model = densenet121_model(img_rows=img_rows, img_cols=img_cols, color_type=channel, num_classes=num_classes)\n",
    "    filepath=\"bestmodel.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    # Start Fine-tuning\n",
    "    history=model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=nb_epoch,\n",
    "              shuffle=True,\n",
    "              verbose=1,\n",
    "              validation_data=(X_valid, Y_valid),\n",
    "              callbacks=callbacks_list,\n",
    "              )\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    # Cross-entropy loss score\n",
    "    score = log_loss(Y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfce4d4-dd2e-4b8b-af4c-3d0b718d0f17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
